{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import gc\nimport os\nimport time\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.utils.data\n\ntqdm.pandas()\n\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TEXT_COL  = \"comment_text\"\nEMB_PATH = '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\ntrain = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv', index_col='id')\ntest = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv', index_col='id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_coefs(word,*arr):return word,np.asanyarray(arr,dtype=\"float32\")\n\ndef load_embeddings(embed_dir=EMB_PATH):\n    embedding_index = dict(get_coefs(*o.strip().split())for o in tqdm(open(embed_dir)))\n    return embedding_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_embedding_matrix(word_index,embeddings_index,max_feature,lower=True,verbose=True):\n    embedding_matrix = np.zeros((max_feature,300))\n    for word,i in tqdm(word_index.items(),disable=not verbose):\n        if lower:\n            word = word.lower()\n        if i>=max_feature:continue\n        try:\n            embedding_vector = embeddings_index[word]\n        except:\n            embedding_vector = embedding_index[\"unknown\"]\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n    return embedding_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_matrix(word_index,embedding_index):\n    embedding_matrix = np.zeros((len(word_index)+1,300))\n    for word,i in word_index.items():\n        try:\n            embedding_matrix[i] = embedding_index[word]\n        except:\n            embedding_matrix[i] = embedding_index[\"unknown\"]\n    return embedding_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"maxlen = 220\nmax_features = 100000\nembed_size = 300\ntokenizer = Tokenizer(num_words=max_features,lower = True)\nprint('fitting tokenizer')\ntokenizer.fit_on_texts(list(train[TEXT_COL])+list(test[TEXT_COL]))\nword_index = tokenizer.word_index\nX_train = tokenizer.texts_to_sequences(list(train[TEXT_COL]))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"target\"] = train[\"target\"].apply(lambda x:1 if x>0.5 else 0)\ny_train= train['target'].values\nX_test = tokenizer.texts_to_sequences(list(test[TEXT_COL]))\nX_train = pad_sequences(X_train,maxlen=maxlen)\nX_test = pad_sequences(X_test,maxlen=maxlen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del tokenizer\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_index = load_embeddings()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = build_matrix(word_index,embedding_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del embedding_index\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Attention(nn.Module):\n    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n        super(Attention, self).__init__(**kwargs)\n        \n        self.supports_masking = True\n\n        self.bias = bias\n        self.feature_dim = feature_dim\n        self.step_dim = step_dim\n        self.features_dim = 0\n        \n        weight = torch.zeros(feature_dim, 1)\n        nn.init.xavier_uniform_(weight)\n        self.weight = nn.Parameter(weight)\n        \n        if bias:\n            self.b = nn.Parameter(torch.zeros(step_dim))\n        \n    def forward(self, x, mask=None):\n        feature_dim = self.feature_dim\n        step_dim = self.step_dim\n\n        eij = torch.mm(\n            x.contiguous().view(-1, feature_dim), \n            self.weight\n        ).view(-1, step_dim)\n        \n        if self.bias:\n            eij = eij + self.b\n            \n        eij = torch.tanh(eij)\n        a = torch.exp(eij)\n        \n        if mask is not None:\n            a = a * mask\n\n        a = a / torch.sum(a, 1, keepdim=True) + 1e-10\n\n        weighted_input = x * torch.unsqueeze(a, -1)\n        return torch.sum(weighted_input, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class NeuralNet(nn.Module):\n    def __init__(self):\n        super(NeuralNet,self).__init__()\n        \n        hidden_size = 64\n        self.embedding = nn.Embedding(max_features,embed_size)\n        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix,dtype=torch.float32))\n        self.embedding.weight.requires_grad = False\n        \n        self.embedding_dropout = nn.Dropout2d(0.2)\n        self.lstm = nn.GRU(embed_size,hidden_size,bidirectional=True,batch_first = True)\n        self.lstm_attention = Attention(hidden_size*2,maxlen)\n        \n        self.out = nn.Linear(384,1)\n    \n    def forward(self,x):\n        h_embedding = self.embedding(x)\n        h_embedding = self.embedding_dropout(h_embedding.transpose(1,2).unsqueeze(-1)).squeeze().transpose(1,2)\n        \n        h_lstm,_ = self.lstm(h_embedding)\n        h_lstm_atten = self.lstm_attention(h_lstm)\n        \n        avg_pool = torch.mean(h_lstm,1)\n        max_pool,_ = torch.max(h_lstm,1)\n        \n        conc = torch.cat((h_lstm_atten,avg_pool,max_pool),1)\n        out = self.out(conc)\n        \n        return out\ndef sigmoid(x):\n    return 1/(1+np.exp(-x))\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Stolen from https://github.com/Bjarten/early-stopping-pytorch\nclass EarlyStopping:\n    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n    def __init__(self, patience=7, verbose=False):\n        \"\"\"\n        Args:\n            patience (int): How long to wait after last time validation loss improved.\n                            Default: 7\n            verbose (bool): If True, prints a message for each validation loss improvement. \n                            Default: False\n        \"\"\"\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = np.Inf\n\n    def __call__(self, val_loss, model):\n\n        score = -val_loss\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n        elif score < self.best_score:\n            self.counter += 1\n            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model):\n        '''Saves model when validation loss decrease.'''\n        if self.verbose:\n            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n        torch.save(model.state_dict(), 'checkpoint.pt')\n        self.val_loss_min = val_loss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nsplits = list(KFold(n_splits = 5).split(X_train,y_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(splits[1][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"splits[1][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 2048\nNUM_EPOCHS = 100\n\ntrain_preds = np.zeros((len(X_train)))\ntest_preds = np.zeros((len(X_test)))\n\nx_test_cuda = torch.tensor(X_test,dtype=torch.long).cuda()\ntest = torch.utils.data.TensorDataset(x_test_cuda)\ntest_loader = torch.utils.data.DataLoader(test,batch_size=BATCH_SIZE,shuffle=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i,(train_idx,valid_idx) in enumerate(splits):\n    x_train_fold = torch.tensor(X_train[train_idx],dtype=torch.long).cuda()\n    y_train_fold = torch.tensor(y_train[train_idx,np.newaxis],dtype=torch.float32).cuda()\n    x_val_fold = torch.tensor(X_train[valid_idx],dtype=torch.long).cuda()\n    y_val_fold = torch.tensor(y_train[valid_idx,np.newaxis],dtype=torch.float32).cuda()\n    \n    model = NeuralNet()\n    model.cuda()\n    loss_fn = torch.nn.BCEWithLogitsLoss(reduction=\"sum\")\n    optimizer = torch.optim.Adam(model.parameters())\n    \n    train = torch.utils.data.TensorDataset(x_train_fold,y_train_fold)\n    valid = torch.utils.data.TensorDataset(x_val_fold,y_val_fold)\n    \n    train_loader = torch.utils.data.DataLoader(train,batch_size=BATCH_SIZE,shuffle=True)\n    valid_loader = torch.utils.data.DataLoader(valid,batch_size=BATCH_SIZE,shuffle=False)\n    early_stopping = EarlyStopping(patience=3,verbose = True)\n    print(f'Fold of {i+1}')\n    \n    for epoch in range(NUM_EPOCHS):\n        start_time = time.time()\n        \n        model.train()\n        avg_loss = 0.\n        for x_batch,y_batch in tqdm(train_loader,disable=True):\n            optimizer.zero_grad()\n            y_pred = model(x_batch)\n            print(type(y_pred))\n            print(type(y_batch))\n            loss = loss_fn(y_pred,y_batch)\n            loss.backward()\n            optimizer.step()\n            avg_loss += loss.item() / len(train_loader)\n        elapsed_time = time.time() - start_time\n        valid_preds_fold = np.zeros((x_val_fold.size(0)))\n        test_preds_fold = np.zeros(len(X_test))\n        avg_val_loss = 0.\n        \n        for i ,(x_batch,y_batch) in enumerate(valid_loader):\n            y_pred = model(x_batch).detach()\n            avg_val_loss += loss_fn(y_pred,y_batch).item()/len(valid_loader)\n            valid_preds_fold[i*BATCH_SIZE:(i+1)*BATCH_SIZE] = sigmoid(y_pred.cpu().numpy())[:,0]\n        \n        elapsed_time = time.time() - start_time\n        print(\"Epoch{}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t time={:.2f}s\".fpr,at(\n        epoch+1,NUM_EPOCHS,avg_loss,avg_val_loss,elapsed_time))\n        \n        early_stopping(avg_val_loss,model)\n        \n        if early_stopping.early_stop:\n            print(\"Early stopping\")\n            break\n        \n    model.load_state_dict(torch.load('checkpoint.pt'))\n    \n    for i,(x_batch,) in enumerate(test_loader):\n        y_pred = model(x_batch).detach()\n        \n        test_preds_fold[i*BATCH_SIZE:(i+1)*BATCH_SIZE] = sigmoid(y_pred.cpu().numpy())[:,0]\n        \n    train_preds[valid_idx] = valid_preds_fold\n    test_preds += test_preds_fold/len(splits)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nroc_auc_score(y_train>0.5,train_preds)","execution_count":30,"outputs":[{"output_type":"execute_result","execution_count":30,"data":{"text/plain":"0.5"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv', index_col='id')\nsubmission['prediction'] = test_preds\nsubmission.reset_index(drop=False, inplace=True)\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}