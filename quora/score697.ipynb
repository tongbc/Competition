{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# Quora Insincere Questions Classification\n",
    "> Detect toxic content to improve online conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import glob\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "import threading\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "from collections import Counter, OrderedDict\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import Model\n",
    "from keras.layers import *\n",
    "from keras.layers.merge import _Merge\n",
    "from keras.models import *\n",
    "from keras.initializers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import *\n",
    "from keras.regularizers import *\n",
    "from keras import backend as K\n",
    "from keras.legacy import interfaces\n",
    "from keras.engine.topology import Layer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.generic_utils import serialize_keras_object\n",
    "from keras.utils.generic_utils import deserialize_keras_object\n",
    "from keras.utils import multi_gpu_model\n",
    "\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "5859a202c0dbc888ea3f0f5a2513fcf1d0295b58"
   },
   "outputs": [],
   "source": [
    "SEED = 2018\n",
    "# python\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "# random\n",
    "np.random.seed(SEED)\n",
    "tf.set_random_seed(SEED)\n",
    "# tf\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "session_conf.gpu_options.allow_growth = True\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "# data\n",
    "max_features = 95000\n",
    "maxlen = 66\n",
    "cv = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "0504404aea24c5aaac8994c7bbd943f8b7c5360b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape:  (1306122, 3)\n",
      "Test shape:  (56370, 2)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "print(\"Train shape: \", train.shape)\n",
    "print(\"Test shape: \", test.shape)\n",
    "sub = test[['qid']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dccacdff5faae7bbad3f255c47dc43fa834a579f"
   },
   "source": [
    "## 整个文本中非字母和数字的字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "2c7c1def47906e92652895546fd38606ae4f98af"
   },
   "outputs": [],
   "source": [
    "punct1 = [char for line in train.question_text for char in line if not char.isalnum()]\n",
    "punct2 = [char for line in test.question_text for char in line if not char.isalnum()]\n",
    "\n",
    "puncs = set(punct1 + punct2)\n",
    "# 去掉空格字符\n",
    "puncs = set(puncs) - set(' ')\n",
    "# 去掉了string的停用词\n",
    "unpunc = puncs - set(punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ec1d5f3a65786942d268dd6ef59f8a27836f24d0"
   },
   "source": [
    "## 缩写词、无法识别字符、英美拼写不同的词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "743b8c72cb5baf02e5a22d6993cf983cbd127dc9"
   },
   "outputs": [],
   "source": [
    "contraction = { \"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\",\n",
    "                \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",\n",
    "                \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\",\n",
    "                \"haven't\": \"have not\", \"haven ' t\"\"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\",\n",
    "                \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "                \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\", \"i'll've\": \"i will have\",\n",
    "                \"i'm\": \"i am\", \"i've\": \"i have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\",\n",
    "                \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\",\n",
    "                \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\",\n",
    "                \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "                \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\n",
    "                \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n",
    "                \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\",\n",
    "                \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\",\n",
    "                \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\",\n",
    "                \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\",\n",
    "                \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\",\n",
    "                \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\n",
    "                \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\",\n",
    "                \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "                \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\",\n",
    "                \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\",\n",
    "                \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \"what's\": \"what is\",\n",
    "                \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\",\n",
    "                \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\",\n",
    "                \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\",\n",
    "                \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\",\n",
    "                \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "                \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n",
    "                \"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\",\n",
    "                \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n",
    "\n",
    "punc = {\"ँ\": \"\", \"◦\": \"\", \"̆\": \"\", \"✏\": \"\", \"\": \"\", \"ี\": \"\", \"♡\": \"o\", \"△\": \"\", \"⇒\": \"\", \"\u0006\": \"\", \"＄\": \" dollar \",\n",
    "        \"→\": \"\", \"͚\": \"\", \"️\": \"\", \"⟩\": \"\", \"¡\": \"i\", \"್\": \"\", \"‬\": \"\", \"̘\": \"\", \"ា\": \"\", \"¿\": \"?\", \"⧼\": \"\",\n",
    "        \"\u0002\": \"\", \"®\": \" r \", \"ौ\": \"\", \"∼\": \"\", \"َ\": \"\", \"ూ\": \"\", \"”\": \"'\", \"̙\": \"\", \"⋅\": \"\", \"̷\": \"\", \"̓\": \"\", \"、\": \"\",\n",
    "        \"⬇\": \"\", \"̔\": \"\", \"∗\": \"*\", \"͕\": \"\", \"͡\": \"\", \"̿\": \"\", \"‌\": \"\", \"͜\": \"\", \"̦\": \"\", \"\": \"\", \"♨\": \"\", \"̮\": \"\",\n",
    "        \"ௌ\": \"\", \"»\": \" \", \"➡\": \"\", \"̼\": \"\", \"̌\": \"\", \"̢\": \"\", \"？\": \"?\", \"\u0010\": \"\", \"ৃ\": \"\", \"ం\": \"\", \"⊥\": \"\",\n",
    "        \"̧\": \"\", \"ਾ\": \"\", \"》\": \" \", \"ਂ\": \"\", \"ិ\": \"\", \"∨\": \"\", \"ী\": \"\", \"े\": \"\", \"⧽\": \"\", \"⁡\": \"\", \"ु\": \"\",\n",
    "        \"ٌ\": \"\", \"₦\": \" naira \", \"̸\": \"\", \"़\": \"\", \"̃\": \"\", \"\u0017\": \"\", \"͎\": \"\", \"∧\": \"\", \"，\": \"\", \"÷\": \"/\", \"،\": \"\",\n",
    "        \"↓\": \"\", \"✔\": \"\", \"⁠\": \"\", \"¶\": \"\", \"ೋ\": \"\", \"͖\": \"\", \"ে\": \"\", \"☝\": \"\", \"«\": \" \", \"\u0001\": \"\", \"ं\": \"\",\n",
    "        \"《\": \" \", \"ॉ\": \"\", \"）\": \"\", \"͉\": \"\", \"⟨\": \"\", \"\": \"\", \"ْ\": \"\", \"‏\": \"\", \"₱\": \" peso \", \"°\": \"\",\n",
    "        \"͋\": \"\", \"✌\": \"\", \"্\": \"\", \"᠌\": \"\", \"♣\": \"\", \"×\": \"x\", \"ো\": \"\", \"؟\": \"?\", \"˜\": \"\", \"̩\": \"\", \"̱\": \"\",\n",
    "        \"̺\": \"\", \"͔\": \"\", \"▾\": \"\", \"⎛\": \"\", \"ొ\": \"\", \"்\": \"\", \"̊\": \"\", \"̥\": \"\", \"ੁ\": \"\", \"่\": \"\", \"﻿\": \"\", \"˚\": \"\",\n",
    "        \"ా\": \"\", \"ા\": \"\", \"™\": \" tm \", \"ِ\": \"\", \"∈\": \"\", \"⃗\": \"\", \"≅\": \"=\", \"̵\": \"\", \"♭\": \"\", \"ಾ\": \"\", \"；\": \".\",\n",
    "        \"̒\": \"\", \"ி\": \"\", \"´\": \"'\", \"＞\": \">\", \"̣\": \"\", \"ุ\": \"\", \"ّ\": \"\", \"▒\": \"\", \"।\": \"\", \"–\": \"-\", \"∖\": \"\",\n",
    "        \"̰\": \"\", \"ॄ\": \"\", \"‘\": \"'\", \"̶\": \"-\", \"ो\": \"\", \"！\": \"!\", \"☺\": \"\", \"̎\": \"\", \"″\": \"\", \"＝\": \"=\", \"˂\": \"\",\n",
    "        \"਼\": \"\", \"ः\": \"\", \"ֿ\": \"\", \"♏\": \"\", \"¦\": \"\", \"̝\": \"\", \"̈\": \"\", \"́\": \"\", \"‐\": \"-\", \"“\": \"'\", \"ാ\": \"\",\n",
    "        \"≤\": \"<=\", \"ੀ\": \"\", \"\u001b\": \"\", \"\\n\": \"\", \"◌\": \"\", \"ृ\": \"\", \"ு\": \"\", \"ा\": \"\", \"¥\": \" yen \", \"‑\": \"-\",\n",
    "        \"￼\": \"\", \"\u0013\": \"\", \"्\": \"\", \"̭\": \"\", \"\": \"\", \"¬\": \"\", \"͌\": \"\", \"̍\": \"\", \"„\": \"\", \"ី\": \"\", \"•\": \"\", \"↑\": \"\",\n",
    "        \"͘\": \"\", \"\": \"\", \"͇\": \"\", \"̫\": \"\", \"ா\": \"\", \"͛\": \"\", \"︠\": \"\", \"⁻\": \"-\", \"᾽\": \"\", \"ি\": \"\", \"̟\": \"\", \"│\": \"|\",\n",
    "        \"̕\": \"\", \"͊\": \"\", \"̑\": \"\", \"‎\": \"\", \"☁\": \"\", \"ಿ\": \"\", \"ी\": \"\", \"̀\": \"\", \"়\": \"\", \"̐\": \"\", \"☉\": \"\", \"\u001a\": \"\",\n",
    "        \"⚧\": \"\", \"£\": \" pound \", \"・\": \"\", \"⋯\": \"...\", \"−\": \"-\", \"∅\": \" \", \"¸\": \",\", \"̋\": \"\", \"̲\": \"\", \"⎝\": \"\",\n",
    "        \"͆\": \"\", \"〗\": \"]\", \"／\": \"\", \"ั\": \"\", \"：\": \"\", \"ோ\": \"\", \"̽\": \"\", \"©\": \" c \", \"\": \"\", \"്\": \"\", \"ು\": \"\",\n",
    "        \"ు\": \"\", \"్\": \"\", \"ि\": \"\", \"⊨\": \"\", \"̈́\": \"\", \"̚\": \"\", \"̖\": \"\", \"̡\": \"\", \"·\": \".\", \"✅\": \"\", \"ͅ\": \"\",\n",
    "        \"ੰ\": \"\", \"̾\": \"\", \"…\": \"\", \"＾\": \"^\", \"≈\": \"=\", \"—\": \"-\", \"♀\": \"\", \"❤\": \"\", \"્\": \"\", \"ା\": \"\", \"¢\": \"\",\n",
    "        \"⎞\": \"\", \"ె\": \"\", \"​\": \"\", \"̻\": \"\", \"（\": \"\", \"‪\": \"\", \"≠\": \"!=\", \"ॢ\": \"\", \"ં\": \"\", \"〖\": \"[\", \"­\": \"\", \"∂\": \"\",\n",
    "        \"̬\": \"\", \"͐\": \"\", \"\": \"\", \"₊\": \"+\", \"℅\": \"%\", \"̛\": \"\", \"‰\": \"\", \"ਿ\": \"\", \"͈\": \"\", \"́\": \"\", \"͂\": \"\", \"̞\": \"\",\n",
    "        \"ి\": \"\", \"้\": \"\", \"̗\": \"\", \"ു\": \"\", \"\u0003\": \"\", \"’\": \"'\", \"া\": \"\", \"ើ\": \"\", \"\": \"\", \"ះ\": \"\", \"」\": \"]\", \"︡\": \"\",\n",
    "        \"ू\": \"\", \"̳\": \"\", \"ை\": \"\", \"⊂\": \"\", \"∇\": \"\", \"≥\": \">=\", \"̄\": \"\", \"₹\": \" e \", \"̜\": \"\", \"̴\": \"\", \"℃\": \"\",\n",
    "        \"±\": \"+\", \"⌚\": \" time \", \"≡\": \"\", \"̹\": \"\", \"̯\": \"\", \"′\": \"\", \"ీ\": \"\", \"ូ\": \"\", \"－\": \" \", \"「\": \"[\", \"̀\": \"\",\n",
    "        \"¨\": \"'\", \"ॣ\": \"\", \"⦁\": \"\", \"€\": \" euro \", \"❓\": \"?\", \"ู\": \"\", \"͗\": \"\", \"̅\": \"\", \"̂\": \"\", \"͠\": \"\", \"̤\": \"\",\n",
    "        \"្\": \"\", \"̉\": \"\", \"₩\": \"\", \"\": \"\", \"̪\": \"\", \"ै\": \"\", \"∘\": \"\", \"ៃ\": \"\", \"͑\": \"\", \"ំ\": \"\", \"͒\": \"\", \"☹\": \"\",\n",
    "        \"͝\": \"\", \"‛\": \"'\", \"⎠\": \"\", \"¯\": \"\", \"。\": \".\", \"∆\": \"\", \"ി\": \"\", \"̓\": \"\", \"∝\": \"\", \"†\": \"\", \"≱\": \"\", \"²\": \"2\",\n",
    "        \"`\": \"'\", 'à': 'a', '³': '3', 'π': 'pi', \"₁\": \"1\", \"₃\": \"3\", \"₆\": \"6\", \"¼\": \"1/4\", \"⁷\": \"7\", \"¾\": \"3/4\",\n",
    "        \"⁵\": \"5\", \"₅\": \"5\", \"½\": \"1/2\", \"₄\": \"4\", \"⅔\": \"2/3\", \"₂\": \"2\", \"¹\": \"1\"}\n",
    "\n",
    "mispell = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling',\n",
    "           'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor',\n",
    "           'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu': 'youtube ',\n",
    "           'qoura': 'quora', 'quorans': 'quora users', 'quoran': 'quora user', 'sallary': 'salary', 'whta': 'what',\n",
    "           'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much',\n",
    "           'howmany': 'how many', 'whydo': 'why do', 'doi': 'do i', 'thebest': 'the best', 'howdoes': 'how does',\n",
    "           'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating',\n",
    "           'pennis': 'penis', 'etherium': 'ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data',\n",
    "           '2k15': '2015', '2k16': '2016', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend',\n",
    "           'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization',\n",
    "           'demonitization': 'demonetization', 'demonetisation': 'demonetization', 'pokémon': 'pokemon',\n",
    "           'nanodegree': 'nano degree', 'brexit': 'british exit', 'cryptocurrencies': 'crypto currencies',\n",
    "           'coinbase': 'coin base', 'oneplus': 'one plus', 'redmi': 'red mi', 'GDPR': 'general data protection regulation',\n",
    "           'DCEU': 'dc extended universe', 'litecoin': 'lite coin', 'unacademy': 'non academy', 'altcoin': 'bitcoin alternative',\n",
    "           'altcoins': 'bitcoin alternative', 'sjw': 'social justice warriors', 'sjws': 'social justice warriors',\n",
    "           'fiancé': 'fiance', 'microservices': 'micro services', 'bitconnect': 'bit connect', 'codeforces': 'code forces',\n",
    "           'wannacry': 'wanna cry', 'onedrive': 'one drive', 'airpods': 'air pods', 'twinflame': 'twin flame',\n",
    "           'undergraduation': 'under graduation', 'cos2x': 'cos 2 x', 'yourquote': 'your quote', 'xiomi': 'xiaomi',\n",
    "           'undertale': 'under tale', 'genderfluid': 'gender fluid', 'são': 'sao', 'chapterwise': 'chapter wise',\n",
    "           'deepmind': 'deep mind', '': '', 'arrowverse': 'arrow verse', 'overbrace': ' ', 'tensorflow': 'tensor flow',\n",
    "           'hackerrank': 'hacker rank', 'microservice': 'micro service', 'reactjs': 'react js', 'hackerearth': 'hacker earth',\n",
    "           'fiancée': 'fiance', 'blockchains': 'block chains', 'beyoncé': 'beyonce', 'neuralink': 'neura link',\n",
    "           'openai': 'open ai', 'zoomcar': 'zoom car', 'hyperconjugation': 'hyper conjugation', 'autoencoder': 'auto encoder',\n",
    "           'webassembly': 'web assembly', 'quoras': 'quora', 'digilocker': 'digi locker', 'oversmart': 'over smart',\n",
    "           'cryptocoins': 'crypto coins', 'crytocurrencies': 'cryto currencies', 'cyrptocurrency': 'cyrpto currency',\n",
    "           'café': 'cafe', 'whatapp': 'whatsapp', 'gaslighter': 'gas lighter', 'darkweb': 'dark web', 'webnovel': 'web novel'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c7a05155224b1bfdb15a655cc89a8ce618e68323"
   },
   "source": [
    "## 数据预处理：\n",
    "* 添加特征：单词数、字母数、大写字母数、标点符号数、平均单词长度等特征\n",
    "* 字符串全部变小写\n",
    "* ‘’‘’这四个标点符号替换为'\n",
    "* 对缩写和英美差别的词进行替换\n",
    "* 将标点符号替换为 空格+标点符号+空格（以免有些词和标点符号无法划分开）\n",
    "* 将数字替换为##..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "a9203c8ca2e4197288e57555184ac571e3ebfca1"
   },
   "outputs": [],
   "source": [
    "def replace_quote(text):\n",
    "    # 替换标点符号\n",
    "    quote = ['´', '‘', '’', \"`\"]\n",
    "    for s in quote:\n",
    "        text = text.replace(s, \"'\")\n",
    "    return text\n",
    "                      \n",
    "def re_mapping(mapping):\n",
    "    res = re.compile('(%s)' % '|'.join(mapping.keys()))\n",
    "    return res\n",
    "\n",
    "# 两个字典取并集\n",
    "mapping = dict(set(contraction.items()) | set(mispell.items()))\n",
    "re_map = re_mapping(mapping)\n",
    "re_punc = re_mapping(punc)\n",
    "\n",
    "def replace_mapping(text):\n",
    "    def replace(match):\n",
    "        return mapping[match.group(0)]\n",
    "    return re_map.sub(replace, text)\n",
    "\n",
    "def replace_punc(text):\n",
    "    def replace(match):\n",
    "        return punc[match.group(0)]\n",
    "    return re_punc.sub(replace, text)\n",
    "\n",
    "def sep_punc(x):\n",
    "    # 将“标点符号”用“ 标点符号 ”来代替\n",
    "    for p in puncs:\n",
    "        x = x.replace(p, f' {p} ')\n",
    "    return x\n",
    "\n",
    "def replace_numbers(x):\n",
    "    x = re.sub('[0-9]{5,}', '#####', x)\n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x\n",
    "\n",
    "def add_features(df):\n",
    "    df['question_text'] = df['question_text'].progress_apply(lambda x: str(x))\n",
    "    # 字符串的长度，字母的数量\n",
    "    df['num_chars'] = df['question_text'].progress_apply(len)\n",
    "    # 单词的数量\n",
    "    df['num_words'] = df.question_text.str.count('\\S+')\n",
    "    # 字符串中大写字母的数量\n",
    "    df['num_capital'] = df['question_text'].progress_apply(lambda x: sum(1 for c in x if c.isupper()))\n",
    "    # 大写字母数占总字母数的比率\n",
    "    df['capital_rate'] = df['num_capital'] / df['num_chars']\n",
    "    \n",
    "    # 不重复单词的种数\n",
    "    df['num_uniquewords'] = df['question_text'].progress_apply(lambda x: len(set(x.split())))\n",
    "    df['unique_rate'] = df['num_uniquewords'] / df['num_words']\n",
    "    \n",
    "    # istitle()字符串中所有单词首字母大写则为真，也就是统计首字母大写的单次数\n",
    "    df[\"num_titlewords\"] = df[\"question_text\"].progress_apply(lambda x: len([w for w in x.split() if w.istitle()]))\n",
    "    # 词频\n",
    "    df['title_rate'] = df['num_titlewords'] / df['num_words']\n",
    "    \n",
    "    # 字符串中所有字母大写则为真\n",
    "    df[\"num_upperwords\"] = df[\"question_text\"].progress_apply(lambda x: len([w for w in x.split() if w.isupper()]))\n",
    "    df['upper_rate'] = df['num_upperwords'] / df['num_words']\n",
    "    \n",
    "    # 统计“！”的数目\n",
    "    df[\"num_exc\"] = df[\"question_text\"].progress_apply(lambda x: x.count(\"!\")).astype('uint16')\n",
    "    # 统计“？”的数目\n",
    "    df[\"num_q\"] = df['question_text'].progress_apply(lambda x: x.count(\"?\")).astype('uint16')\n",
    "    # 单词长度的平均值\n",
    "    df[\"mean_word_len\"] = df[\"question_text\"].progress_apply(lambda x: np.mean([len(w) for w in x.split()]))\n",
    "    # 单词长度的最大值\n",
    "    df[\"max_word_len\"] = df['question_text'].progress_apply(lambda x: max([len(w) for w in x.split()]))\n",
    "    # 特殊字符的数目\n",
    "    df[\"num_unpunc\"] = df[\"question_text\"].progress_apply(lambda x: sum(x.count(p) for p in unpunc)).astype('uint16')\n",
    "    df[\"num_punc\"] = df[\"question_text\"].progress_apply(lambda x: sum(x.count(p) for p in punctuation)).astype('uint16')\n",
    "    # 错拼词的数目\n",
    "    df[\"num_mispell\"] = df[\"question_text\"].progress_apply(lambda x: sum(x.count(p) for p in mispell)).astype('uint16')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "bf6f04db6ec37680d49c1772d0d8c0877ffdc4ee"
   },
   "outputs": [],
   "source": [
    "# 0.699 f10\n",
    "# feature_cols = ['capital_rate', 'num_chars', 'num_words', \"max_word_len\", \"mean_word_len\",\n",
    "#                 'num_capital', \"num_punc\", 'num_uniquewords', \"num_q\", \"num_unpunc\"]\n",
    "\n",
    "# 0.697 f13\n",
    "# feature_cols = ['capital_rate', 'unique_rate', 'num_chars', 'num_words', \"max_word_len\", \"mean_word_len\",\n",
    "#                 'num_capital', \"num_punc\", 'num_uniquewords', \"num_q\", \"num_unpunc\", \"num_exc\", \"num_mispell\"]\n",
    "\n",
    "# 0.697 f12\n",
    "# feature_cols = ['capital_rate', 'num_chars', 'num_words', \"max_word_len\", \"mean_word_len\",\n",
    "#                 'num_capital', \"num_punc\", 'num_uniquewords', \"num_q\", \"num_unpunc\", \"num_exc\", \"num_mispell\"]\n",
    "\n",
    "# f11\n",
    "feature_cols = ['capital_rate', 'num_chars', 'num_words', \"max_word_len\", \"mean_word_len\", 'num_capital',\n",
    "                \"num_punc\", 'num_uniquewords', \"num_q\", \"num_unpunc\", \"num_exc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower():\n",
    "# Lower\n",
    "    train[\"question_text\"] = train[\"question_text\"].str.lower()\n",
    "    test[\"question_text\"] = test[\"question_text\"].str.lower()\n",
    "    print(\"Lower done\")\n",
    "\n",
    "def Replace_quote():\n",
    "# Replace quote\n",
    "    train['question_text'] = train['question_text'].apply(lambda x: replace_quote(x))\n",
    "    test['question_text'] = test['question_text'].apply(lambda x: replace_quote(x))\n",
    "    print(\"Replace quote done\")\n",
    "\n",
    "def Replace_mapping():\n",
    "# Replace mapping(contraction & mispell)\n",
    "    train['question_text'] = train['question_text'].apply(lambda x: replace_mapping(x))\n",
    "    test['question_text'] = test['question_text'].apply(lambda x: replace_mapping(x))\n",
    "    print(\"Replace mapping done\")\n",
    "\n",
    "def Sep_punc():\n",
    "# # Sep punc\n",
    "    train['question_text'] = train['question_text'].apply(lambda x: sep_punc(x))\n",
    "    test['question_text'] = test['question_text'].apply(lambda x: sep_punc(x))\n",
    "    print(\"Sep punc done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replace quote done\n"
     ]
    }
   ],
   "source": [
    "threads = []\n",
    "\n",
    "t2 = threading.Thread(target=Replace_quote)\n",
    "t3 = threading.Thread(target=Replace_mapping)\n",
    "t4 = threading.Thread(target=Sep_punc)\n",
    "threads = [t2, t3, t4]\n",
    "for t in threads:\n",
    "    t.setDaemon(True)\n",
    "    t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "46d9da75748279cc417c05d5e28baeb2acf48fcf"
   },
   "outputs": [],
   "source": [
    "# Add features\n",
    "train = add_features(train)\n",
    "test = add_features(test)\n",
    "features = train[feature_cols].fillna(0)\n",
    "test_features = test[feature_cols].fillna(0)\n",
    "# 对添加的特征进行归一化\n",
    "ss = StandardScaler()\n",
    "ss.fit(np.vstack((features, test_features)))\n",
    "features = ss.transform(features)\n",
    "test_features = ss.transform(test_features)\n",
    "print(\"Add features done\")\n",
    "\n",
    "def lower():\n",
    "# Lower\n",
    "    train[\"question_text\"] = train[\"question_text\"].str.lower()\n",
    "    test[\"question_text\"] = test[\"question_text\"].str.lower()\n",
    "    print(\"Lower done\")\n",
    "\n",
    "def Replace_quote():\n",
    "# Replace quote\n",
    "    train['question_text'] = train['question_text'].progress_apply(lambda x: replace_quote(x))\n",
    "    test['question_text'] = test['question_text'].progress_apply(lambda x: replace_quote(x))\n",
    "    print(\"Replace quote done\")\n",
    "\n",
    "def Replace_mapping():\n",
    "# Replace mapping(contraction & mispell)\n",
    "    train['question_text'] = train['question_text'].progress_apply(lambda x: replace_mapping(x))\n",
    "    test['question_text'] = test['question_text'].progress_apply(lambda x: replace_mapping(x))\n",
    "    print(\"Replace mapping done\")\n",
    "\n",
    "def Sep_punc():\n",
    "# # Sep punc\n",
    "    train['question_text'] = train['question_text'].progress_apply(lambda x: sep_punc(x))\n",
    "    test['question_text'] = test['question_text'].progress_apply(lambda x: sep_punc(x))\n",
    "    print(\"Sep punc done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "49355a19064800b798e16ad0defbd4da88bd1e1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replace mapping done\n",
      "Sep punc done\n"
     ]
    }
   ],
   "source": [
    "train_X = train['question_text']\n",
    "test_X = test['question_text']\n",
    "# filters注意设置，因为标点符号被保留\n",
    "tokenizer = Tokenizer(num_words=max_features, filters='')\n",
    "tokenizer.fit_on_texts(train_X.tolist())\n",
    "train_X = tokenizer.texts_to_sequences(train_X)\n",
    "train_X = pad_sequences(train_X, maxlen=maxlen)\n",
    "\n",
    "test_X = tokenizer.texts_to_sequences(test_X)\n",
    "test_X = pad_sequences(test_X, maxlen=maxlen)\n",
    "train_y = train['target'].values\n",
    "# 通过word_index与Embedding实现一一对应\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "dbe4ec9bc0da74d50b8982a5811961653bf4fb41"
   },
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def load_glove():\n",
    "    \n",
    "    vec_path = \"../input/embeddings/glove.840B.300d/glove.840B.300d.txt\"\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(vec_path, encoding='latin'))\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    emb_size = all_embs.shape[1]\n",
    "\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, emb_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "def load_para():\n",
    "    vec_path =  \"../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt\"\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(vec_path, encoding='latin'))\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    emb_size = all_embs.shape[1]\n",
    "\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, emb_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "204e20d9b7ab3f066c86d98fd16724825ff428ab"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-014c5b15fbd0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_glove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_para\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mthreads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-d64362aab9fb>\u001b[0m in \u001b[0;36mload_glove\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mvec_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"../input/embeddings/glove.840B.300d/glove.840B.300d.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0membeddings_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_coefs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mall_embs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "t1 = load_glove()\n",
    "t2 = load_para()\n",
    "threads = [t1, t2]\n",
    "\n",
    "for t in threads:\n",
    "    t.setDaemon(True)\n",
    "    t.start()\n",
    "\n",
    "total_time = (time.time() - start_time) / 60\n",
    "print(\"Took {:.2f} minutes\".format(total_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_glove = load_glove()\n",
    "emb_para =load_para()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = np.mean([emb_glove, emb_para], axis=0)\n",
    "print(np.shape(emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d2f773aeca53bb260bae2c0984d3f114f42c9be1"
   },
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim\n",
    "    \n",
    "# A Capsule Implement with Pure Keras\n",
    "class Capsule(Layer):\n",
    "    def __init__(self, num_capsule, dim_capsule, routings=3, kernel_size=(9, 1), share_weights=True,\n",
    "                 activation='default', **kwargs):\n",
    "        super(Capsule, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.kernel_size = kernel_size\n",
    "        self.share_weights = share_weights\n",
    "        if activation == 'default':\n",
    "            self.activation = squash\n",
    "        else:\n",
    "            self.activation = Activation(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(Capsule, self).build(input_shape)\n",
    "        input_dim_capsule = input_shape[-1]\n",
    "        if self.share_weights:\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(1, input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     # shape=self.kernel_size,\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "        else:\n",
    "            input_num_capsule = input_shape[-2]\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(input_num_capsule,\n",
    "                                            input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "\n",
    "    def call(self, u_vecs):\n",
    "        if self.share_weights:\n",
    "            u_hat_vecs = K.conv1d(u_vecs, self.W)\n",
    "        else:\n",
    "            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n",
    "\n",
    "        batch_size = K.shape(u_vecs)[0]\n",
    "        input_num_capsule = K.shape(u_vecs)[1]\n",
    "        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n",
    "                                            self.num_capsule, self.dim_capsule))\n",
    "        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n",
    "        # final u_hat_vecs.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
    "\n",
    "        b = K.zeros_like(u_hat_vecs[:, :, :, 0])  # shape = [None, num_capsule, input_num_capsule]\n",
    "        for i in range(self.routings):\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))  # shape = [None, input_num_capsule, num_capsule]\n",
    "            c = K.softmax(b)\n",
    "            c = K.permute_dimensions(c, (0, 2, 1))\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))\n",
    "            outputs = self.activation(K.batch_dot(c, u_hat_vecs, [2, 2]))\n",
    "            if i < self.routings - 1:\n",
    "                b = K.batch_dot(outputs, u_hat_vecs, [2, 3])\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.num_capsule, self.dim_capsule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "233d4b20d2eb81aa63007aa831cf1441884fc67e"
   },
   "outputs": [],
   "source": [
    "def f1_smart(y_true, y_pred):\n",
    "    # y_pred的排序索引\n",
    "    args = np.argsort(y_pred)\n",
    "    # 为1的数量\n",
    "    tp = y_true.sum()\n",
    "    \n",
    "    fs = (tp - np.cumsum(y_true[args[:-1]])) / np.arange(y_true.shape[0] + tp - 1, tp, -1)\n",
    "    res_idx = np.argmax(fs)\n",
    "    return 2 * fs[res_idx], (y_pred[args[res_idx]] + y_pred[args[res_idx + 1]]) / 2\n",
    "\n",
    "def threshold_search(y_true, y_pred):\n",
    "    best_threshold = 0\n",
    "    best_score = 0\n",
    "    for threshold in [i * 0.01 for i in range(25,45)]:\n",
    "        score = f1_score(y_true=y_true, y_pred=(y_pred > threshold).astype(int))\n",
    "        if score > best_score:\n",
    "            best_threshold = threshold\n",
    "            best_score = score\n",
    "    return best_score, best_threshold\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "class CyclicLR(Callback):\n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma**(x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "        \n",
    "    def clr(self):\n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
    "            \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        \n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b79ee814a8461900c064c53d064bc053494e524f"
   },
   "outputs": [],
   "source": [
    "class LstmAtn():\n",
    "    def model(self, embedding_matrix, maxlen, max_features):\n",
    "        inp = Input(shape=(maxlen,))\n",
    "        emb_size = embedding_matrix.shape[1]\n",
    "        x_emb = Embedding(max_features, emb_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "        x = SpatialDropout1D(0.5)(x_emb)\n",
    "        x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x)\n",
    "        y = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\n",
    "\n",
    "        atn_1 = Attention(maxlen)(x)\n",
    "        atn_2 = Attention(maxlen)(y)\n",
    "        avg_pool = GlobalAveragePooling1D()(y)\n",
    "        max_pool = GlobalMaxPooling1D()(y)\n",
    "        x = concatenate([atn_1, atn_2, avg_pool, max_pool])\n",
    "        x = Dense(16, activation=\"relu\")(x)\n",
    "        x = Dropout(0.1)(x)\n",
    "        output = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "        model = Model(inputs=inp, outputs=output)\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy',f1])\n",
    "        return model\n",
    "\n",
    "class LstmFAtnCap():\n",
    "    def model(self, embedding_matrix, maxlen, max_features):\n",
    "        inp_seq = Input(shape=(maxlen,), name='seq')\n",
    "        inp_feature = Input(shape=(len(feature_cols),), name='feature')\n",
    "        emb_size = embedding_matrix.shape[1]\n",
    "        x_emb = Embedding(max_features, emb_size, weights=[embedding_matrix], trainable=False)(inp_seq)\n",
    "        \n",
    "        x = SpatialDropout1D(0.2)(x_emb)\n",
    "        x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x)\n",
    "        y = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\n",
    "\n",
    "        # last = Lambda(lambda t: t[:, -1])(y)\n",
    "        atn_1 = Attention(maxlen)(y)\n",
    "        atn_2 = Attention(maxlen)(y)\n",
    "        avg_pool = GlobalAveragePooling1D()(y)\n",
    "        max_pool = GlobalMaxPooling1D()(y)\n",
    "        \n",
    "        capsule = Capsule(num_capsule=10, dim_capsule=16, routings=5,\n",
    "                          share_weights=True)(x)\n",
    "        capsule = Flatten()(capsule)\n",
    "        capsule = Dropout(0.25)(capsule)\n",
    "        \n",
    "        x = concatenate([atn_1, atn_2, avg_pool, max_pool, inp_feature, capsule])\n",
    "        x = Dense(16, activation='relu', kernel_initializer=glorot_normal(seed=SEED))(x)\n",
    "        x = Dropout(0.1)(x)\n",
    "\n",
    "        output = Dense(1, activation=\"sigmoid\")(x)\n",
    "        model = Model(inputs=[inp_seq, inp_feature], outputs=output)\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy',f1])\n",
    "        return model\n",
    "\n",
    "    \n",
    "class LstmFAtn():\n",
    "    def model(self, embedding_matrix, maxlen, max_features):\n",
    "        inp_seq = Input(shape=(maxlen,), name='seq')\n",
    "        inp_feature = Input(shape=(len(feature_cols),), name='feature')\n",
    "        emb_size = embedding_matrix.shape[1]\n",
    "        x_emb = Embedding(max_features, emb_size, weights=[embedding_matrix], trainable=False)(inp_seq)\n",
    "        \n",
    "        x = SpatialDropout1D(0.2)(x_emb)\n",
    "        x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x)\n",
    "        y = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\n",
    "\n",
    "        # last = Lambda(lambda t: t[:, -1])(y)\n",
    "        atn_1 = Attention(maxlen)(y)\n",
    "        atn_2 = Attention(maxlen)(y)\n",
    "        avg_pool = GlobalAveragePooling1D()(y)\n",
    "        max_pool = GlobalMaxPooling1D()(y)\n",
    "\n",
    "        x = concatenate([atn_1, atn_2, avg_pool, max_pool, inp_feature])\n",
    "        x = Dense(32, activation='relu', kernel_initializer=glorot_normal(seed=SEED))(x)\n",
    "        x = Dropout(0.1)(x)\n",
    "\n",
    "        output = Dense(1, activation=\"sigmoid\")(x)\n",
    "        model = Model(inputs=[inp_seq, inp_feature], outputs=output)\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy',f1])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "140480d4ee48d6fa0603cdc8e8fca4c6c45a7d6e"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def train_single():\n",
    "    X_train, X_val, y_train, y_val = train_test_split(train_X, train_y, test_size=0.2,random_state=2018, stratify=train_y)\n",
    "    model = LstmAtn().model(embed_glove, maxlen, max_features)\n",
    "    history = model.fit(X_train, y_train, batch_size=256, epochs=5, validation_data=(X_val, y_val),)\n",
    "    pred_glove_val_y = model.predict([X_val], batch_size=1024, verbose=1)\n",
    "    best_threshold = 0\n",
    "    best_score = 0\n",
    "\n",
    "    for threshold in [i * 0.01 for i in range(25,45)]:\n",
    "        score = metrics.f1_score(y_val, (pred_glove_val_y>threshold).astype(int))\n",
    "        if score > best_score:\n",
    "            best_threshold = threshold\n",
    "            best_score = score\n",
    "    print('best score:%f,best threshold:%f'%(best_score, best_threshold))\n",
    "    pred_test_y = model.predict(test_X, batch_size=1024, verbose=1)\n",
    "    pred_test_y = (pred_test_y > best_threshold).astype(int)\n",
    "    gc.collect()\n",
    "    K.clear_session()\n",
    "    tf.reset_default_graph()\n",
    "    return pred_test_y\n",
    "\n",
    "def train_cv():\n",
    "    kfolds, epochs = 5, 5\n",
    "    kf = StratifiedKFold(n_splits=kfolds, random_state=26, shuffle=True).split(train_X, train_y)\n",
    "    loss = []\n",
    "    thresh = []\n",
    "    # 词向量与特征向量融合，注意区分\n",
    "    train_meta = np.zeros((train_X.shape[0],1))\n",
    "    test_meta = np.zeros((test_X.shape[0],1))\n",
    "    x_test = [test_X, test_features]\n",
    "    for i, (train_idx, valid_idx) in enumerate(kf):\n",
    "\n",
    "        X_train, X_val, Y_train, Y_val = train_X[train_idx], train_X[valid_idx], train_y[train_idx], train_y[valid_idx]\n",
    "        features_train = features[train_idx]\n",
    "        features_val= features[valid_idx]\n",
    "        \n",
    "        x_train = [X_train, features_train]\n",
    "        x_val = [X_val, features_val]\n",
    "        # 模型定义\n",
    "        model = LstmFAtn().model(emb, maxlen, max_features)\n",
    "        if i == 0: print(model.summary())\n",
    "        print('====================================Fold:%d========================================'%(i+1))\n",
    "        # early stopping callbacks\n",
    "        early_stop = EarlyStopping(monitor=\"val_f1\", mode=\"max\", patience=5, verbose=True,restore_best_weights=True)\n",
    "#         clr = CyclicLR(base_lr=0.001, max_lr=0.003, step_size=300, mode='exp_range',gamma=0.99994)\n",
    "        history = model.fit(x_train, Y_train, batch_size=512, epochs=epochs, \n",
    "                            validation_data=(x_val, Y_val), callbacks=[early_stop])\n",
    "        \n",
    "        pred_val_y = model.predict(x_val, batch_size=1024,)\n",
    "        \n",
    "        best_threshold = 0\n",
    "        best_score = 0\n",
    "        for threshold in [i * 0.01 for i in range(25,45)]:\n",
    "            score = metrics.f1_score(Y_val, (pred_val_y>threshold).astype(int))\n",
    "            if score > best_score:\n",
    "                best_threshold = threshold\n",
    "                best_score = score\n",
    "        print('best score:%f,best threshold:%f'%(best_score, best_threshold))\n",
    "        \n",
    "        pred_test_y = model.predict(x_test, batch_size=1024,)\n",
    "        \n",
    "        train_meta[valid_idx] = pred_val_y\n",
    "        test_meta += pred_test_y / kfolds\n",
    "\n",
    "#     best_score, best_thresh = f1_smart(train_y, train_meta)\n",
    "#     print(best_score, best_thresh)\n",
    "    best_score1,best_thresh1 = threshold_search(train_y, train_meta)\n",
    "    print(best_score1, best_thresh1)\n",
    "    # best_score, best_thresh = threshold_search(np.squeeze(Y), train_meta)\n",
    "#     print('Optimal F1: {:.4f} at threshold: {:.4f}'.format(best_score, best_thresh))\n",
    "#     print('mean_thresh: {:.4f} and mean_loss: {:.4f}'.format(np.mean(thresh), np.mean(loss)))\n",
    "    test_meta = test_meta.reshape((-1, 1))\n",
    "    pred_test_y = (test_meta > best_thresh1).astype(int)\n",
    "    return pred_test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3e0b0c3fdd6b6a0830e1d01631ca57783169a20d"
   },
   "outputs": [],
   "source": [
    "pred_test_y = train_cv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "62c8e0a0748aa0e11465572e6d156d9f28578508"
   },
   "outputs": [],
   "source": [
    "submit = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "submit['prediction'] = pred_test_y\n",
    "submit.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "04c9bcb810289e94ba9bd6beaaee09a38766a4c5"
   },
   "source": [
    "训练日志：\n",
    "* Optimal F1: 0.6874 at threshold: 0.3162 - val_loss: 0.0965\n",
    "* Optimal F1: 0.6879 at threshold: 0.3589- val_loss: 0.0965\n",
    "* Optimal F1: 0.6945 at threshold: 0.3551 - val_loss: 0.0947\n",
    "* Optimal F1: 0.6903 at threshold: 0.3207- val_loss: 0.0951\n",
    "* Optimal F1: 0.6859 at threshold: 0.3639- val_loss: 0.0966\n",
    "* 所有数据：Optimal F1: 0.6888 at threshold: 0.3612\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1364c1aff83ce48aae9e42b15ad3b5a6dfa50b2a"
   },
   "source": [
    "### epoch = 7    batch_size = 512\n",
    "best score:0.689091,best threshold:0.330000  \n",
    "best score:0.694275,best threshold:0.430000  \n",
    "best score:0.691881,best threshold:0.340000  \n",
    "best score:0.691157,best threshold:0.410000  \n",
    "best score:0.685592,best threshold:0.440000  \n",
    "0.6893022238341134 0.37    \n",
    "\n",
    "### epoch = 5 batch_size = 512\n",
    "best score:0.692113,best threshold:0.440000  \n",
    "best score:0.688417,best threshold:0.310000  \n",
    "best score:0.693517,best threshold:0.320000  \n",
    "best score:0.693974,best threshold:0.430000  \n",
    "best score:0.687383,best threshold:0.430000  \n",
    "0.6895025813324592 0.4  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
