{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# Quora Insincere Questions Classification\n",
    "> 0.699 finally\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import glob\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "from collections import Counter, OrderedDict\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import Model\n",
    "from keras.layers import *\n",
    "from keras.layers.merge import _Merge\n",
    "from keras.models import *\n",
    "from keras.initializers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import *\n",
    "from keras.regularizers import *\n",
    "from keras import backend as K\n",
    "from keras.legacy import interfaces\n",
    "from keras.engine.topology import Layer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.generic_utils import serialize_keras_object\n",
    "from keras.utils.generic_utils import deserialize_keras_object\n",
    "from keras.utils import multi_gpu_model\n",
    "\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5859a202c0dbc888ea3f0f5a2513fcf1d0295b58",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SEED = 2018\n",
    "# python\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "# random\n",
    "np.random.seed(SEED)\n",
    "tf.set_random_seed(SEED)\n",
    "# tf\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "session_conf.gpu_options.allow_growth = True\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "# data\n",
    "max_features = 95000\n",
    "maxlen = 72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0504404aea24c5aaac8994c7bbd943f8b7c5360b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "print(\"Train shape: \", train.shape)\n",
    "print(\"Test shape: \", test.shape)\n",
    "sub = test[['qid']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cf2d36d78cdcf7bbebffacfbbe973b9c583cc6eb",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_emb(filename):\n",
    "    def get_coefs(word, *arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "    if \"wiki-news-300d-1M.vec\" in filename:\n",
    "        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(filename) if len(o)>100)\n",
    "    else:\n",
    "        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(filename, encoding='latin'))\n",
    "    return embeddings_index\n",
    "\n",
    "glove = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "para = \"../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt\"\n",
    "print(\"Extracting embedding\")\n",
    "embeddings_index_glove = load_emb(glove)\n",
    "embeddings_index_para = load_emb(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cd566cd9d1af7282547efa6c3079a386ad17eed8",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocab(texts):\n",
    "    sentences = texts.apply(lambda x: x.split()).values\n",
    "    vocab = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab\n",
    "\n",
    "def check_coverage(vocab, embeddings_index):\n",
    "    known_words = {}\n",
    "    unknown_words = {}\n",
    "    nb_known_words = 0\n",
    "    nb_unknown_words = 0\n",
    "    for word in vocab.keys():\n",
    "        try:\n",
    "            known_words[word] = embeddings_index[word]\n",
    "            nb_known_words += vocab[word]\n",
    "        except:\n",
    "            unknown_words[word] = vocab[word]\n",
    "            nb_unknown_words += vocab[word]\n",
    "    print('Found embeddings for {:.3%} of vocab'.format(len(known_words) / len(vocab)))\n",
    "    print('Found embeddings for  {:.3%} of all text'.format(nb_known_words / (nb_known_words + nb_unknown_words)))\n",
    "    unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n",
    "    return unknown_words\n",
    "\n",
    "def add_lower(embedding, vocab):\n",
    "    count = 0\n",
    "    for word in vocab:\n",
    "        if word in embedding and word.lower() not in embedding:  \n",
    "            embedding[word.lower()] = embedding[word]\n",
    "            count += 1\n",
    "    print(f\"Added {count} words to embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "69bc3e8f365e1f6a1b7e445ab9495ebf5103ee03",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get set of all punctuations in dataset\n",
    "tmp = []\n",
    "for x in train.question_text:\n",
    "    for c in x:\n",
    "        if not c.isalnum():\n",
    "            tmp.append(c)\n",
    "for x in test.question_text:\n",
    "    for c in x:\n",
    "        if not c.isalnum():\n",
    "            tmp.append(c)\n",
    "puncs = set(tmp) - set(' ')\n",
    "unpunc = puncs - set(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "743b8c72cb5baf02e5a22d6993cf983cbd127dc9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "contraction = { \"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\",\n",
    "                \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",\n",
    "                \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\",\n",
    "                \"haven't\": \"have not\", \"haven ' t\"\"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\",\n",
    "                \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "                \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\", \"i'll've\": \"i will have\",\n",
    "                \"i'm\": \"i am\", \"i've\": \"i have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\",\n",
    "                \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\",\n",
    "                \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\",\n",
    "                \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "                \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\n",
    "                \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n",
    "                \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\",\n",
    "                \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\",\n",
    "                \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\",\n",
    "                \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\",\n",
    "                \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\",\n",
    "                \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\n",
    "                \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\",\n",
    "                \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "                \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\",\n",
    "                \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\",\n",
    "                \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \"what's\": \"what is\",\n",
    "                \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\",\n",
    "                \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\",\n",
    "                \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\",\n",
    "                \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\",\n",
    "                \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "                \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n",
    "                \"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\",\n",
    "                \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n",
    "\n",
    "mispell = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling',\n",
    "           'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor',\n",
    "           'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu': 'youtube ',\n",
    "           'qoura': 'quora', 'quorans': 'quora users', 'quoran': 'quora user', 'sallary': 'salary', 'whta': 'what',\n",
    "           'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much',\n",
    "           'howmany': 'how many', 'whydo': 'why do', 'doi': 'do i', 'thebest': 'the best', 'howdoes': 'how does',\n",
    "           'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating',\n",
    "           'pennis': 'penis', 'etherium': 'ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data',\n",
    "           '2k15': '2015', '2k16': '2016', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend',\n",
    "           'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization',\n",
    "           'demonitization': 'demonetization', 'demonetisation': 'demonetization', 'pokémon': 'pokemon',\n",
    "           'nanodegree': 'nano degree', 'brexit': 'british exit', 'cryptocurrencies': 'crypto currencies',\n",
    "           'coinbase': 'coin base', 'oneplus': 'one plus', 'redmi': 'red mi', 'GDPR': 'general data protection regulation',\n",
    "           'DCEU': 'dc extended universe', 'litecoin': 'lite coin', 'unacademy': 'non academy', 'altcoin': 'bitcoin alternative',\n",
    "           'altcoins': 'bitcoin alternative', 'sjw': 'social justice warriors', 'sjws': 'social justice warriors',\n",
    "           'fiancé': 'fiance', 'microservices': 'micro services', 'bitconnect': 'bit connect', 'codeforces': 'code forces',\n",
    "           'wannacry': 'wanna cry', 'onedrive': 'one drive', 'airpods': 'air pods', 'twinflame': 'twin flame',\n",
    "           'undergraduation': 'under graduation', 'cos2x': 'cos 2 x', 'yourquote': 'your quote', 'xiomi': 'xiaomi',\n",
    "           'undertale': 'under tale', 'genderfluid': 'gender fluid', 'são': 'sao', 'chapterwise': 'chapter wise',\n",
    "           'deepmind': 'deep mind', '': '', 'arrowverse': 'arrow verse', 'overbrace': ' ', 'tensorflow': 'tensor flow',\n",
    "           'hackerrank': 'hacker rank', 'microservice': 'micro service', 'reactjs': 'react js', 'hackerearth': 'hacker earth',\n",
    "           'fiancée': 'fiance', 'blockchains': 'block chains', 'beyoncé': 'beyonce', 'neuralink': 'neura link',\n",
    "           'openai': 'open ai', 'zoomcar': 'zoom car', 'hyperconjugation': 'hyper conjugation', 'autoencoder': 'auto encoder',\n",
    "           'webassembly': 'web assembly', 'quoras': 'quora', 'digilocker': 'digi locker', 'oversmart': 'over smart',\n",
    "           'cryptocoins': 'crypto coins', 'crytocurrencies': 'cryto currencies', 'cyrptocurrency': 'cyrpto currency',\n",
    "           'café': 'cafe', 'whatapp': 'whatsapp', 'gaslighter': 'gas lighter', 'darkweb': 'dark web', 'webnovel': 'web novel'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a9203c8ca2e4197288e57555184ac571e3ebfca1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_quote(text):\n",
    "    quote = ['´', '‘', '’', \"`\"]\n",
    "    for s in quote:\n",
    "        text = text.replace(s, \"'\")\n",
    "    return text\n",
    "                      \n",
    "def re_mapping(mapping):\n",
    "    res = re.compile('(%s)' % '|'.join(mapping.keys()))\n",
    "    return res\n",
    "\n",
    "mapping = dict(set(contraction.items()) | set(mispell.items()))\n",
    "re_map = re_mapping(mapping)\n",
    "def replace_mapping(text):\n",
    "    def replace(match):\n",
    "        return mapping[match.group(0)]\n",
    "    return re_map.sub(replace, text)\n",
    "\n",
    "def sep_punc(x):\n",
    "    for p in puncs:\n",
    "        x = x.replace(p, f' {p} ')\n",
    "    return x\n",
    "\n",
    "def replace_numbers(x):\n",
    "    x = re.sub('[0-9]{5,}', '#####', x)\n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x\n",
    "\n",
    "def add_features(df):\n",
    "    df['question_text'] = df['question_text'].progress_apply(lambda x: str(x))\n",
    "    df['num_chars'] = df['question_text'].progress_apply(len)\n",
    "    df['num_words'] = df.question_text.str.count('\\S+')\n",
    "\n",
    "    df['num_capital'] = df['question_text'].progress_apply(lambda x: sum(1 for c in x if c.isupper()))\n",
    "    df['capital_rate'] = df['num_capital'] / df['num_words']\n",
    "\n",
    "    df['num_uniquewords'] = df['question_text'].progress_apply(lambda x: len(set(x.split())))\n",
    "    df[\"num_exc\"] = df[\"question_text\"].progress_apply(lambda x: x.count(\"!\")).astype('uint16')\n",
    "    df[\"num_q\"] = df['question_text'].progress_apply(lambda x: x.count(\"?\")).astype('uint16')\n",
    "    df[\"num_,\"] = df['question_text'].progress_apply(lambda x: x.count(\",\")).astype('uint16')\n",
    "    df[\"num_.\"] = df['question_text'].progress_apply(lambda x: x.count(\".\")).astype('uint16')\n",
    "    df[\"mean_word_len\"] = df[\"question_text\"].progress_apply(lambda x: np.mean([len(w) for w in x.split()]))\n",
    "    df[\"max_word_len\"] = df['question_text'].progress_apply(lambda x: max([len(w) for w in x.split()]))\n",
    "\n",
    "    df[\"num_unpunc\"] = df[\"question_text\"].progress_apply(lambda x: sum(x.count(p) for p in unpunc)).astype('uint16')\n",
    "    df[\"num_punc\"] = df[\"question_text\"].progress_apply(lambda x: sum(x.count(p) for p in punctuation)).astype('uint16')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bf6f04db6ec37680d49c1772d0d8c0877ffdc4ee",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_cols = ['capital_rate', 'num_chars', 'num_words', \"max_word_len\", \"mean_word_len\",\n",
    "                'num_capital', \"num_punc\", 'num_uniquewords', \"num_q\", \"num_unpunc\", \"num_exc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "46d9da75748279cc417c05d5e28baeb2acf48fcf",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add features\n",
    "train = add_features(train)\n",
    "test = add_features(test)\n",
    "\n",
    "features = train[feature_cols].fillna(0)\n",
    "test_features = test[feature_cols].fillna(0)\n",
    "ss = StandardScaler()\n",
    "ss.fit(np.vstack((features, test_features)))\n",
    "features = ss.transform(features)\n",
    "test_features = ss.transform(test_features)\n",
    "print(\"Add features done\")\n",
    "\n",
    "vocab = build_vocab(train['question_text'])\n",
    "\n",
    "# Lower\n",
    "train[\"question_text\"] = train[\"question_text\"].str.lower()\n",
    "test[\"question_text\"] = test[\"question_text\"].str.lower()\n",
    "print(\"Lower done\")\n",
    "\n",
    "# Add lower word to embedding:\n",
    "add_lower(embeddings_index_glove, vocab)\n",
    "add_lower(embeddings_index_para, vocab)\n",
    "\n",
    "# Replace quote\n",
    "train['question_text'] = train['question_text'].progress_apply(lambda x: replace_quote(x))\n",
    "test['question_text'] = test['question_text'].progress_apply(lambda x: replace_quote(x))\n",
    "print(\"Replace quote done\")\n",
    "\n",
    "# Replace mapping(contraction & mispell)\n",
    "train['question_text'] = train['question_text'].progress_apply(lambda x: replace_mapping(x))\n",
    "test['question_text'] = test['question_text'].progress_apply(lambda x: replace_mapping(x))\n",
    "print(\"Replace mapping done\")\n",
    "\n",
    "# Sep punc\n",
    "train['question_text'] = train['question_text'].progress_apply(lambda x: sep_punc(x))\n",
    "test['question_text'] = test['question_text'].progress_apply(lambda x: sep_punc(x))\n",
    "print(\"Sep punc done\")\n",
    "\n",
    "# Replace numbers\n",
    "train['question_text'] = train['question_text'].progress_apply(lambda x: replace_numbers(x))\n",
    "test['question_text'] = test['question_text'].progress_apply(lambda x: replace_numbers(x))\n",
    "print(\"Replace numbers done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e34f9f3887ca7bf6393000c81d733e008264ba09",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = train[\"question_text\"].fillna(\"_na_\").values\n",
    "T_X = test[\"question_text\"].fillna(\"_na_\").values\n",
    "tokenizer = Tokenizer(num_words=max_features, filters='')\n",
    "tokenizer.fit_on_texts(X.tolist() + T_X.tolist())\n",
    "X = tokenizer.texts_to_sequences(X)\n",
    "X = pad_sequences(X, maxlen=maxlen)\n",
    "T_X = tokenizer.texts_to_sequences(T_X)\n",
    "T_X = pad_sequences(T_X, maxlen=maxlen)\n",
    "Y = train['target'].values\n",
    "word_index = tokenizer.word_index\n",
    "print(len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "614fc7dda6c7e93e0fd8a43400c73ce844c858dc",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del train, test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dbe4ec9bc0da74d50b8982a5811961653bf4fb41",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def build_emb(embeddings_index, max_features, word_index):\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    emb_size = all_embs.shape[1]\n",
    "\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, emb_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "204e20d9b7ab3f066c86d98fd16724825ff428ab",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_glove = build_emb(embeddings_index_glove, max_features, word_index)\n",
    "emb_para = build_emb(embeddings_index_para, max_features, word_index)\n",
    "emb = np.mean([emb_glove, emb_para], axis=0)\n",
    "print(np.shape(emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d2f773aeca53bb260bae2c0984d3f114f42c9be1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim\n",
    "\n",
    "\n",
    "class AttentivePooling(Layer):\n",
    "    def __init__(self, W_regularizer=None, b_regularizer=None, **kwargs):\n",
    "        self.supports_masking = False\n",
    "        # self.mask =mask\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "        super(AttentivePooling, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        n_in = input_shape[2]\n",
    "        n_out = 1\n",
    "        lim = np.sqrt(6. / (n_in + n_out))\n",
    "        # tanh initializer xavier\n",
    "        self.W = K.random_uniform_variable((n_in, n_out), -lim, lim,\n",
    "                                           name='{}_W'.format(self.name))\n",
    "        self.b = K.zeros((n_out,), name='{}_b'.format(self.name))\n",
    "        self.trainable_weights = [self.W, self.b]\n",
    "        self.regularizer = []\n",
    "        if self.W_regularizer is not None:\n",
    "            self.add_loss(self.W_regularizer(self.W))\n",
    "        if self.b_regularizer is not None:\n",
    "            self.add_loss(self.b_regularizer(self.b))\n",
    "        self.build = True\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "\n",
    "        memory = inputs\n",
    "        print('memory shape', K.int_shape(memory))\n",
    "        gi = K.tanh(K.dot(memory, self.W) + self.b)  # 32 *6 *1\n",
    "        gi = K.sum(gi, axis=-1)  # 32 *6\n",
    "        alfa = K.softmax(gi)\n",
    "        self.alfa = alfa\n",
    "        output = K.sum(memory * K.expand_dims(alfa, axis=-1), axis=1)  # sum(32 *6 *310)\n",
    "        print('output shape', K.int_shape(output))\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        shape = input_shape\n",
    "        shape = list(shape)\n",
    "\n",
    "        return (shape[0], shape[2])\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "233d4b20d2eb81aa63007aa831cf1441884fc67e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f1_smart(y_true, y_pred):\n",
    "    args = np.argsort(y_pred)\n",
    "    tp = y_true.sum()\n",
    "    fs = (tp - np.cumsum(y_true[args[:-1]])) / np.arange(y_true.shape[0] + tp - 1, tp, -1)\n",
    "    res_idx = np.argmax(fs)\n",
    "    return 2 * fs[res_idx], (y_pred[args[res_idx]] + y_pred[args[res_idx + 1]]) / 2\n",
    "\n",
    "def threshold_search(y_true, y_pred):\n",
    "    best_threshold = 0\n",
    "    best_score = 0\n",
    "    for threshold in [i * 0.01 for i in range(100)]:\n",
    "        score = f1_score(y_true=y_true, y_pred=(y_pred > threshold).astype(int))\n",
    "        if score > best_score:\n",
    "            best_threshold = threshold\n",
    "            best_score = score\n",
    "    return best_score, best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b79ee814a8461900c064c53d064bc053494e524f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LstmAtn():\n",
    "    def model(self, embedding_matrix, maxlen, max_features):\n",
    "        inp = Input(shape=(maxlen,))\n",
    "        emb_size = embedding_matrix.shape[1]\n",
    "        x_emb = Embedding(max_features, emb_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "        x = SpatialDropout1D(0.2)(x_emb)\n",
    "        x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x)\n",
    "        y = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\n",
    "\n",
    "        atn_1 = Attention(maxlen)(x)\n",
    "        atn_2 = Attention(maxlen)(y)\n",
    "        avg_pool = GlobalAveragePooling1D()(y)\n",
    "        max_pool = GlobalMaxPooling1D()(y)\n",
    "        x = concatenate([atn_1, atn_2, avg_pool, max_pool])\n",
    "        x = Dense(16, activation=\"relu\")(x)\n",
    "        x = Dropout(0.1)(x)\n",
    "        output = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "        model = Model(inputs=inp, outputs=output)\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "class LstmFAtn():\n",
    "    def model(self, embedding_matrix, maxlen, max_features):\n",
    "        inp_seq = Input(shape=(maxlen,), name='seq')\n",
    "        inp_feature = Input(shape=(len(feature_cols),), name='feature')\n",
    "        emb_size = embedding_matrix.shape[1]\n",
    "        x_emb = Embedding(max_features, emb_size, weights=[embedding_matrix], trainable=False)(inp_seq)\n",
    "        x = SpatialDropout1D(0.2)(x_emb)\n",
    "        x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x)\n",
    "        y = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\n",
    "\n",
    "        atn_1 = Attention(maxlen)(x)\n",
    "        atn_2 = Attention(maxlen)(y)\n",
    "        avg_pool = GlobalAveragePooling1D()(y)\n",
    "        max_pool = GlobalMaxPooling1D()(y)\n",
    "\n",
    "        x = concatenate([atn_1, atn_2, avg_pool, max_pool, inp_feature])\n",
    "        x = Dense(32, activation='relu', kernel_initializer=glorot_normal(seed=SEED))(x)\n",
    "        x = Dropout(0.1)(x)\n",
    "\n",
    "        output = Dense(1, activation=\"sigmoid\")(x)\n",
    "        model = Model(inputs=[inp_seq, inp_feature], outputs=output)\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0eef9ba31fbe859e232403f12f93465c443d873a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "warmup = False\n",
    "models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "da80a30c1ffdf1f03efcbab99bc1b94246a80057",
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_pred(model, epochs, X_train, X_val, T_X, Y_train, Y_val, mm=False):\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=1, min_lr=0.0001, verbose=2)\n",
    "    filepath = \"best_weights.h5\"\n",
    "    logloss = ModelCheckpoint(filepath, monitor='val_loss', verbose=2, save_best_only=True, mode='min')\n",
    "    if warmup:\n",
    "        warm_up = WarmUp()\n",
    "        callbacks = [logloss, warm_up, reduce_lr]\n",
    "    else:\n",
    "        callbacks = [logloss, reduce_lr]\n",
    "    history = model.fit(X_train, Y_train, batch_size=512, epochs=epochs, validation_data=(X_val, Y_val), verbose=2, callbacks=callbacks)\n",
    "    best_loss = np.min(history.history['val_loss'])\n",
    "    model.load_weights(filepath)\n",
    "    pred_val_y = np.squeeze(model.predict(X_val, batch_size=1024, verbose=2))\n",
    "    pred_test_y = np.squeeze(model.predict(T_X, batch_size=1024, verbose=2))\n",
    "    best_score, best_thresh = f1_smart(Y_val, pred_val_y)\n",
    "    print('Optimal F1: {:.4f} at threshold: {:.4f}'.format(best_score, best_thresh))\n",
    "\n",
    "    del model\n",
    "    gc.collect()\n",
    "    K.clear_session()\n",
    "    tf.reset_default_graph()\n",
    "    return pred_val_y, pred_test_y, best_score, best_thresh, best_loss\n",
    "\n",
    "kfolds, epochs = 5, 5\n",
    "kf = StratifiedKFold(n_splits=kfolds, random_state=26, shuffle=True).split(X, Y)\n",
    "loss = []\n",
    "thresh = []\n",
    "train_meta = np.zeros(Y.shape)\n",
    "test_meta = np.zeros(T_X.shape[0])\n",
    "x_test = [T_X, test_features]\n",
    "\n",
    "for i, (train_idx, valid_idx) in enumerate(kf):\n",
    "    X_train, X_val, Y_train, Y_val = X[train_idx], X[valid_idx], Y[train_idx], Y[valid_idx]\n",
    "    features_train = features[train_idx]\n",
    "    features_val= features[valid_idx]\n",
    "    x_train = [X_train, features_train]\n",
    "    x_val = [X_val, features_val]\n",
    "\n",
    "    model = LstmFAtn().model(emb, maxlen, max_features)\n",
    "    if i == 0: print(model.summary())\n",
    "    pred_val_y, pred_test_y, best_score, best_thresh, best_loss = train_pred(model, epochs, x_train, x_val, x_test, Y_train, Y_val)\n",
    "    loss.append(best_loss)\n",
    "    thresh.append(best_thresh)\n",
    "    train_meta[valid_idx] = pred_val_y\n",
    "    test_meta += pred_test_y / kfolds\n",
    "\n",
    "best_score, best_thresh = f1_smart(np.squeeze(Y), train_meta)\n",
    "print('Optimal F1: {:.4f} at threshold: {:.4f}'.format(best_score, best_thresh))\n",
    "print('mean_thresh: {:.4f} and mean_loss: {:.4f}'.format(np.mean(thresh), np.mean(loss)))\n",
    "test_meta = test_meta.reshape((-1, 1))\n",
    "pred_test_y = (test_meta > best_thresh).astype(int)\n",
    "\n",
    "sub['prediction'] = pred_test_y\n",
    "sub.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cb942b3cb7d22ed87baf4192d6ef8860884e9322",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_time = (time.time() - start_time)\n",
    "print(\"Took {:.2f} seconds\".format(total_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
